\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage[all]{xy}
\usepackage{amsmath,amsthm,amssymb,color,latexsym}
\usepackage{geometry}        
\geometry{letterpaper}    
\usepackage{graphicx}
\usepackage{minted}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}              % set the Output
\SetKw{KwRet}{return}
\newtheorem{problem}{Problem}

\newenvironment{solution}[1][\it{Solution}]{\textbf{#1: } }


\begin{document}
\pagestyle{fancy}
\fancyhf{}
\rhead{COSC 6342 \\ \today}
\chead{Assignment 1 \\ Supervised Learning}
\lhead{Machine Learning \\ \textit{Student Name: Vedant Vohra  }}
\rfoot{Page \thepage}
\lfoot{University of Houston}

\noindent Due Date: \textcolor{red}{Friday, September 23, 2022, 11:59 PM}
\begin{abstract}
    In this assignment, you will learn about linear regression and classification. The questions below can be answered with the help of the book. Show all calculations done step-by-step. In problem 3, you need not worry about being "mathematically" correct.
    Please follow the same notation as in the book. You are allowed to work on teams of up to three people. If working as a team, every member of the team should submit the (same) report. All reports and supplemental material must be zipped as \texttt{team\#\_Assignment\#.zip} and uploaded on Blackboard.
\end{abstract}

\begin{problem}

\end{problem}

\begin{solution}
Both shrinkage methods and feature selection share a common goal, which is to simplify the model and to reduce the variance, to potentially improve the overall accuracy. Feature selection works by reducing the feature space to a subset of features and eliminating the rest. Thereby reducing the variance of the model at the cost of increased bias. Shrinkage methods, on the other hand, attempt to reduce the size of the coefficients by applying a constraint or penalty on them. Shrinkage methods can be used for feature selection if the constraint reduces some of the coefficients’ values to 0.

Feature selection is a discrete process, since there is a hard threshold on the feature set and some features are eliminated. This results in higher variance and can negatively impact the overall model accuracy. In comparison, ridge regression is more continuous and hence, results in a smoother model (i.e. lower variance).

The equation for ridge regression is: 
\begin{equation*} \hat{\beta}^{ridge} = (X^TX+\lambda I)^{-1} X^Ty \end{equation*}

Since the equation is linear, we can calculate the variance as:
\begin{equation*} Var(\hat{\beta}^{ridge}) = \sigma^2 (X^TX+\lambda I)^{-1} X^TX (X^TX+\lambda I)^{-1} \end{equation*}

From the above equation, it is clear that as the value of $\lambda$ is increased, the variance decreases.

\end{solution}

\begin{problem}

\begin{solution}
Three assumptions of linear regression are as follows:
1] Linearity: If there exists a linear relationship between the predictor variable (Assume x) and the response variable (Assume y) it satisfies the linearity rule. The linearity can be checked by plotting the variables on a scatter plot, if both the points roughly form a straight line then there should exist some linearity between the points. 

2] Homoscedasticity: Through linear regression, the error variance in the residuals should always be constant to satisfy the rule of Homoscedasticity. If not, the regression model would suffer from Heteroscedasticity. The best way to identify is to plot a residual vs fitted values plot and observe as the residuals become more spread the fitted values become larger, indicating a violation of Homoscedasticity and Independence.

3] Normality – When the errors/residuals are normally distributed i.e. when plotted preferably using a Q-Q plot the points should have a rough diagonal distribution to satisfy the normality rule.
\\
\begin{bmatrix}
1 & 2 & 3\\
2 & 3 & 5\\
3 & 4 & 7\\
4 & 5 & 9\\
\end{bmatrix}
$C1 \rightarrow C1 + 1$
\\
\begin{bmatrix}
2 & 2 & 3\\
3 & 3 & 5\\
4 & 4 & 7\\
5 & 5 & 9\\
\end{bmatrix}
$C1 \rightarrow C1 - C2$
\\
\begin{bmatrix}
0 & 2 & 3\\
0 & 3 & 5\\
0 & 4 & 7\\
0 & 5 & 9\\
\end{bmatrix}
$C2 \rightarrow C2 - C3$
\\
\begin{bmatrix}
0 & -1 & 3\\
0 & -2 & 5\\
0 & -3 & 7\\
0 & -4 & 9\\
\end{bmatrix}
$C2 \rightarrow 3C2 + C3$
\\
\begin{bmatrix}
0 & 0 & 3\\
0 & -1 & 5\\
0 & -2 & 7\\
0 & -3 & 9\\
\end{bmatrix}
$C2 \rightarrow 5C2
\\
\begin{bmatrix}
0 & 0 & 3\\
0 & -5 & 5\\
0 & -10 & 7\\
0 & -15 & 9\\
\end{bmatrix}
$C3 \rightarrow C3 + C2$
\\
\begin{bmatrix}
0 & 0 & 3\\
0 & -5 & 0\\
0 & -10 & -3\\
0 & -15 & -6\\
\end{bmatrix}
$C2 \rightarrow C2\div-5$ \& $C3 \rightarrow C3\div3$
\\
\begin{bmatrix}
0 & 0 & 1\\
0 & 1 & 0\\
0 & 2 & -1\\
0 & 3 & -2\\
\end{bmatrix}






Based on the p(A) = 2, i.e. column rank of a matrix obtained. We could say that it is a deficient matrix and it does affect the linear regression. The column rank means the number of non-zero or independent columns of the matrix.

The rank of a matrix can be written as the product column vector times row vector. If c1 and c2 are column vectors, matrix c1c2$^{T}$ is rank of the matrix.So we will require 2n-1 elements to form a matrix.

In a matrix when A∈Rm×n is of rank r, then we can represent A as
c1c2$^{T}$  where $c1$\in$ m\times r \& c2$\in$ n\times r$. 
\\So we will require $(2n\times p-p{^2})$ elements to form a matrix. 

\\Since, the rank of our matrix A is 2, we may have to compress and perform efficient matrix operations.

We can perform Singular Value Decomposition as a compression technique on our matrix.

\end{solution}

\begin{problem}

\end{problem}
\end{problem}

\begin{solution}

\end{solution}

\begin{problem}

\end{problem}

\begin{solution}
Feature selection is an essential part of the machine learning process where the input variables are narrowed down to improve efficiency and computational cost of a model. The number of these input variables can be reduced by identifying the variables that are most important or relevant to the model and the task.
\\
\\
There are three types of feature selection methods:\\
\emph{Wrapper methods}: These methods follow an iterative process to narrow down the features. For example, in forward selection, the method begins with no features. It then adds significant features one by one based on p-values until it obtains the most efficient model with the most relevant features. Forward selection follows the greedy algorithm. In backward selection however, the model starts with all features and eliminates insignificant features based on p-values until an efficient model is obtained. However, these iterative processes have a higher computational cost, resulting in a major drawback.\\
\emph{Filter methods}: These methods select a subset of features by studying the relationship of features with the dependent variable using statistical tests. While these methods reduce computational cost by avoiding iterative processes, they do not account for possible interactions between features. Pearson correlation, ANOVA and variance thresholding are examples of filter methods. \\
\emph{Embedded methods}: These methods are known as embedded methods as they refer to machine learning algorithms that have the feature selection algorithm embedded in them. This means that the algorithm simultaneously performs the subset selection and classification/regression task, displaying better computational cost than wrapper methods and more accuracy than filter methods. An example of an embedded method is ridge regression. In ridge regression, the degree of correlation of features can be varied. There may be certain features that aren't as relevant or significant as other features but cannot be removed completely. The correlation of these features can thus be reduced using ridge regression. Thus embedded methods are a good balance between wrapper methods and filter methods, avoiding high computation costs by avoiding the iterative process and also ensuring interactions between features are considered.
\\
\\
Proposed feature selection method:
\begin{itemize}
    \item Conducting a survey using experts in the task domain to obtain subset of features relevant to the task
    \item Eliminate insignificant interaction terms and insignificant predictors based on statistical tests such as the t-test
    \item Use ridge regression to further scale the correlation of final feature set to improve performance of the model
\end{itemize}
\\
The advantages of the proposed feature selection method is that it avoids the high computation cost of wrapper methods, thus reducing the time taken. It also ensures that correlations between features are not ignored and are given relative importance to other features that may have more of a significant relation to the dependent variable, enduring a higher prediction accuracy.

\end{solution}

\begin{problem}

\end{problem}

\begin{solution}
The Newton Rhapson algorithm is used to approximate the roots of a non-linear function by using the second order derivatives of the coefficients. These derivatives are generated using an n*n (n = number of coefficients) matrix known as a Hessian matrix. 

Calculating second derivatives is a computationally complex task. For a model with 1000 features, this would require the computation of 1000 * 1000 = 1,000,000 second derivatives. Moreover, the resulting Heissian matrix of 1 million floating point numbers would need to be stored in memory.

As the number of features increases, the cost of computing the derivatives increases exponentially and the memory required to store the matrix increases quadratically. Hence, the Newton Raphson algorithm is not suitable for use with a large number of features.

\end{solution}

\begin{problem}

\end{problem}

\begin{solution}
Linear Discriminant analysis functions correctly only when the predictor variables are continuous and not categorical, following Normal Distribution. Whereas, in logistic regression categorical variables can be used to make possible predictions.
 
They vary in estimating the method of coefficients. Logistic and linear regression can predict the probability of classification of a case into a dependent variables group. Every observation can be classified into a different group based on the assumed cut-off value. As such, the group with the largest categorical values controls the mean and variance values which are considered continuous and normally distributed. Despite making these assumptions considering normality fulfillment and being continuous LDA performance is affected by the logistic regression, since it makes robust assumptions without not assuming about the linear relationship between dependent variables and the homogeneity of the variance. Therefore, when the assumptions are not met, LDA should be avoided and we could analyse the data using logisitic regression.

Since LDA relies on more assumptions when compared to Logistic regression it is more vulnerable to outliers. While logistic regression relies on lesser assumptions it seems to be less vulnerable to outliers.


\end{solution}

\begin{problem}

\end{problem}

\begin{solution}

\end{solution}

\begin{problem}

\end{problem}

\begin{solution}

\end{solution}


\newpage
\begin{thebibliography}{9}
\bibitem{texbook}
Michael M. Meskhi (2022) \emph{An example reference item}, University of Houston.
\bibitem{texbook}
https://online.stat.psu.edu/stat508/lesson/9/9.2/9.2.9
\bibitem{texbook}
https://math.stackexchange.com/questions/21100/importance-of-matrix-rank
\bibitem{texbook}
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2798100/
\end{thebibliography}

\end{document}
